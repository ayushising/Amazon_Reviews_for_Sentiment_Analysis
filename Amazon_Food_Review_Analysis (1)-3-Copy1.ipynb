{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BDA9CDVs1i2D"
   },
   "source": [
    "#Amazon Review Analysis\n",
    "\n",
    "<b> Objective: </b> : To determine whether a review is positive or negative and build based on the rating provided. <br>\n",
    "<b> Data Source :</b> https://www.kaggle.com/snap/amazon-fine-food-reviews/\n",
    "\n",
    "<br> <b>Data Overview :</b> This dataset contains all the reviews written by people who purchaged iffrent food product from the amazon along with their rating and review<br>\n",
    "<br>\n",
    "\n",
    "Number of reviews: 568,454<br>\n",
    "Number of users: 256,059<br>\n",
    "Number of products: 74,258<br>\n",
    "Timespan: Oct 1999 - Oct 2012<br>\n",
    "Number of Attributes/Columns in data: 10 <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-cp_kAxCAY2"
   },
   "source": [
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. Id\n",
    "2. ProductId - unique identifier for the product\n",
    "3. UserId - unqiue identifier for the user\n",
    "4. ProfileName\n",
    "5. HelpfulnessNumerator - number of users who found the review helpful\n",
    "6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n",
    "7. Score - rating between 1 and 5\n",
    "8. Time - timestamp for the review\n",
    "9. Summary - brief summary of the review\n",
    "10. Text - text of the review\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "[Q] How to determine if a review is positive or negative?<br>\n",
    "<br> \n",
    "[Ans] We could use the Score/Rating. A rating of 4 or 5 could be cosnidered a positive review. A review of 1 or 2 could be considered negative. A review of 3 is  ignored as it can be positive or negative and that can impact our model accuracy. This is an approximate and proxy way of determining the polarity (positivity/negativity) of a review.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPGDb9u86Gm4"
   },
   "source": [
    "##Loding Dataset\n",
    "The dataset is in 2 format \n",
    "1. Sqlite Database\n",
    "2. .csv file\n",
    "\n",
    "We will be using Sqlite database as it is more easier to fetch,visulize and do manipulation of data with sql commands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDpPCqFc6yAe"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MaRS4MjhwLb9"
   },
   "outputs": [],
   "source": [
    "con=sqlite3.connect('Amazondatabase.sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5UJGIqA89GGG"
   },
   "source": [
    "**Reading database into dataframe by neglecting all the 3 star reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCf2Fp3t8nhN"
   },
   "outputs": [],
   "source": [
    "data=pd.read_sql_query(\"\"\"\n",
    "SELECT * \n",
    "From reviews\n",
    "where Score !=3\n",
    "\"\"\",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "eaWLLoJ58o84",
    "outputId": "801dc560-24c3-40b0-bc41-f0cd2f3e334a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  ...                                               Text\n",
       "0   1  ...  I have bought several of the Vitality canned d...\n",
       "1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2   3  ...  This is a confection that has been around a fe...\n",
       "3   4  ...  If you are looking for the secret ingredient i...\n",
       "\n",
       "[4 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets See how Our Dataframe Looks Like\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMpq3TSB9-Ql"
   },
   "source": [
    "#Converting Score into positive or negative\n",
    "We are going to change our score in the form of positive and negative<br>\n",
    "Score 1-2 : Negative<br>\n",
    "Score 4-5 : Positive<br>\n",
    "Score 3 : Already Neglected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wq_oLST69hrf"
   },
   "outputs": [],
   "source": [
    "## Writting function to convert score into Positive or Negative\n",
    "## Positive -> 1 and Negative -> 0\n",
    "def convert(x):\n",
    "  if x >3:\n",
    "    return 1\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQBl5sWf_HHX"
   },
   "outputs": [],
   "source": [
    "score=data.Score\n",
    "new=score.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aq2-5ccV_i7_"
   },
   "outputs": [],
   "source": [
    "#new column after geting score converted in Positive or Negative\n",
    "# Updating our score  column with  0 & 1 \\\\ the new colum that we have ceated\n",
    "data.Score=new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "ozB3Mjkw_4GY",
    "outputId": "91e022f8-d18a-4a16-8761-97f275f693d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  ...                                               Text\n",
       "0   1  ...  I have bought several of the Vitality canned d...\n",
       "1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2   3  ...  This is a confection that has been around a fe...\n",
       "3   4  ...  If you are looking for the secret ingredient i...\n",
       "4   5  ...  Great taffy at a great price.  There was a wid...\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tah0XQL0A86J"
   },
   "source": [
    "*Score are coverted into Positve and Negative*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "WFQ4GC_CFCg-",
    "outputId": "52befdc4-7e24-48b6-d03a-93b6c366dc83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 525814 entries, 0 to 525813\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      525814 non-null  int64 \n",
      " 1   ProductId               525814 non-null  object\n",
      " 2   UserId                  525814 non-null  object\n",
      " 3   ProfileName             525814 non-null  object\n",
      " 4   HelpfulnessNumerator    525814 non-null  int64 \n",
      " 5   HelpfulnessDenominator  525814 non-null  int64 \n",
      " 6   Score                   525814 non-null  int64 \n",
      " 7   Time                    525814 non-null  int64 \n",
      " 8   Summary                 525814 non-null  object\n",
      " 9   Text                    525814 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 40.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aSKa3vgfBOGm"
   },
   "source": [
    "##[2] ***Exploratory Data Analysis***\n",
    "#Data Cleaning *(Deduplication)*\n",
    "After having  look into data we discovered that there were many duplicacy in data like <br>\n",
    "Same Person reviewed many product with same time stamp and given similar review and rating. \n",
    "<br>\n",
    "So it is very important to remove those type of duplicate data in order to prevent unbiased results for the analysis of the data. \n",
    "\n",
    "*Lets have look at some duplications*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "NqUm9Aa1A5ZE",
    "outputId": "b15f5414-2dd8-4da6-8a11-cc7cfe57266a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id  ...                                               Text\n",
       "0   78445  ...  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...\n",
       "1  138317  ...  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...\n",
       "2  138277  ...  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...\n",
       "3   73791  ...  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...\n",
       "4  155049  ...  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display= pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score != 3 AND UserId=\"AR5J8UI46CURR\"\n",
    "ORDER BY ProductID\n",
    "\"\"\", con)\n",
    "display.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOh5189vRakk"
   },
   "source": [
    "*Here we can see that same person have reviewdor rated many diffrent product(with diffrent product id at the same timestamp with same review and same rating which is imposible as no once can give review to diffrent products at the same time*<br>\n",
    "\n",
    "**So we need to remove this Duplication to get more acuurate result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4eFOaO4hQcpL"
   },
   "outputs": [],
   "source": [
    "sort_data=data.sort_values(by=['ProductId'],axis=0,ascending =True)\n",
    "clean=data.drop_duplicates(subset={'ProductId','UserId','ProfileName','Score','Time','Summary'},keep='first',inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "RMu9UFU3T31x",
    "outputId": "f1c427c3-89ac-4152-e6d2-e1a2da8c829f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleaning:  (525814, 10)\n",
      "After Cleaning:  (524507, 10)\n",
      "Data Lost : 1307\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Cleaning: \",data.shape)\n",
    "print(\"After Cleaning: \",clean.shape)\n",
    "print(\"Data Lost :\",data.shape[0]-clean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBgAQcFEWhkh"
   },
   "source": [
    "**As we know that the HelpfulnessNumerator is the number of users who found the review helpful and HelpfulnessDenominator is number of users who indicated whether they found the review helpful or not** <br>\n",
    "*So here we can conclude that HelpfulnessDenominator should always be greater than the HelpfulnessNumerator*<br>\n",
    "\n",
    "Lets see if there is any data that is voilating up this rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "UOP3RcN_UP5y",
    "outputId": "159f894c-6fcb-4249-d6f4-32c15deb1a5e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  ...                                               Text\n",
       "0  44737  ...  It was almost a 'love at first bite' - the per...\n",
       "1  64422  ...  My son loves spaghetti so I didn't hesitate or...\n",
       "\n",
       "[2 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display=pd.read_sql_query('''\n",
    "Select * \n",
    "from Reviews\n",
    "where Score !=3 and HelpfulnessDenominator < HelpfulnessNumerator ''',con)\n",
    "display.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kPxwVAQOYlbu"
   },
   "source": [
    "\n",
    "\n",
    "We found two row that is not matching this condition so we need to remove them to\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6wSHVDYqX3ur"
   },
   "outputs": [],
   "source": [
    "final=clean[clean.HelpfulnessDenominator>=clean.HelpfulnessNumerator]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1b9ddqCIYzTt"
   },
   "source": [
    "Now we have perform data cleaning and our cleaned dataframe is saved in **final** datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zL3cdEFVYZO5"
   },
   "outputs": [],
   "source": [
    "text=final.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "DY-HmMa0WR93",
    "outputId": "deb71b91-952f-45d8-cf29-657e6835f1c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         I have bought several of the Vitality canned d...\n",
      "1         Product arrived labeled as Jumbo Salted Peanut...\n",
      "2         This is a confection that has been around a fe...\n",
      "3         If you are looking for the secret ingredient i...\n",
      "4         Great taffy at a great price.  There was a wid...\n",
      "                                ...                        \n",
      "525809    Great for sesame chicken..this is a good if no...\n",
      "525810    I'm disappointed with the flavor. The chocolat...\n",
      "525811    These stars are small, so you can give 10-15 o...\n",
      "525812    These are the BEST treats for training and rew...\n",
      "525813    I am very satisfied ,product is as advertised,...\n",
      "Name: Text, Length: 524505, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "8BQw8KYh8eTm",
    "outputId": "7e9f6350-e557-4771-f863-bc325743da01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                        0\n",
       "ProductId                 0\n",
       "UserId                    0\n",
       "ProfileName               0\n",
       "HelpfulnessNumerator      0\n",
       "HelpfulnessDenominator    0\n",
       "Score                     0\n",
       "Time                      0\n",
       "Summary                   0\n",
       "Text                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for the very last lets check is there any  null value present \n",
    "final.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGWeAJY5sWg-"
   },
   "source": [
    "## [3]Text pre-processing\n",
    "<br> Preprocessing Review Text\n",
    "<br> Now when we have finally removed all the duplicates and all unwanted data now we need to do some pre-processing of our data before creating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "S1WNwJ-nv6wQ",
    "outputId": "c86accae-4f83-4f76-d7c2-3e2e73004e9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I have bought several of the Vitality canned d...\n",
       "1    Product arrived labeled as Jumbo Salted Peanut...\n",
       "2    This is a confection that has been around a fe...\n",
       "3    If you are looking for the secret ingredient i...\n",
       "4    Great taffy at a great price.  There was a wid...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=final.Text\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XdLPHrW-wr7"
   },
   "source": [
    "<b> Lets first check out what type of garbage are there in our text </b> <br>\n",
    "We re going to see the text of random text columm to have a look at our text data garbage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "KogtUUbTX8Rr",
    "outputId": "b1749a31-5b5d-4530-f8e3-10cb9f820efe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found at:  156\n",
      "I have drunk Ricore since my mother allowed be to drink caffeine. I love this stuff in milk.<br />It's much smoother and tastier than instant coffee with milk. I am French and live in the US,<br />and bring back boxes of Ricore at every opportunity. Savora is another hidden gem you can bring<br />back. Check it out.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Found at:  724\n",
      "This is NOT Disco Dust which is edible and made for baking. DO NOT purchase this product for such purposes!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Found at:  529\n",
      "I have never been addicted to anything in my life...until I tasted these chips.  I have tried other brands of the sea salt and vinegar flavor and they are just not the same.  You've got to stick with this blue bag of chips!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Found at:  431\n",
      "These Honey Dijon chips bring a terrific balance of salty, tangy, sweet and crunchy. Other brands I've tried use too much mustard flavor, which overpowers the Honey Dijon pairing.<br /><br />These chips make a great side dish on a BBQ plate with baked beans, coleslaw or potato salad and grilled meat. Naturally, they also go down great with beer.<br /><br />As an Oregonian, I'm proud to share these delectable snacks with friends, especially those living outside our state and who haven't experienced gourmet chips. I tell them Kettle Brand does for potato chips what microbrews did for beer.<br /><br />Kettle Brand potato chips are unmistakable--a light gold color, rich flavor and amazing crunch. Kettle Brand chips are also a healthier snacking option than the major chip brands. Kettle Brand chips don't have trans fats, MSG or artificial flavors and colorings. The company also has a line of organic potato chips and all of their products are certified Kosher.<br /><br />I also recommend these other Kettle Chips flavors: <a href=\"http://www.amazon.com/gp/product/B000G6MBUA\">Kettle Chips Sea Salt & Vinegar</a> and <a href=\"http://www.amazon.com/gp/product/B000G6Q4GM\">Kettle Chips Spicy Thai</a>.<br /><br />Annette Solomon, a reporter for the Salem Statesman Journal recently noted that a glass of wine goes nicely with these chips. Solomon wrote, \"...you could be missing out on a wonderful pairing. These chips are spicy, so you would want to select a semi-sweet white wine. Also, a moderate amount of acid will subdue the strong flavors of ginger, lime, garlic and cilantro without over-powering them. Classically, a German-style Riesling fits these parameters perfectly.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Found at:  540\n",
      "When I saw the Spicy Thai chips, I knew i had to try them. I love spicy and I love Thai food, so why not? First off, these chips are not super spicy. They actually taste sort of sweet at first, with many degrees of flavor. The spiciness is more of a lingering taste that bites afterwards. I'd say these are definitely a good, unique chip. But if you absolutely hate sweet chips, I'd try a different flavor since these are a little sweet.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i=0\n",
    "while i <5:\n",
    "    \n",
    "    ran=random.randint(1,1000)\n",
    "    print(\"Found at: \",ran)\n",
    "    print(text.values[ran])\n",
    "    print('-'*100)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IyxedzpKzF_r"
   },
   "source": [
    "Here we can see that some of our text have **HTML** tag  within it<br>\n",
    "So lets remove all the html tags using simple regualr expression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dl45SMIh_nmK"
   },
   "source": [
    "There might also be url as people somtimes refer another product.<br>\n",
    "So lets remove all url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6T4H_HB_jZ8"
   },
   "outputs": [],
   "source": [
    "## function to remove URl\n",
    "def removeUrl(text):\n",
    "  pr=re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "  return pr\n",
    "## we also use Beautifull Soup to do this more efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7aR7zOPBkfJ"
   },
   "source": [
    "Now we need to de concatinate words like:<br>\n",
    "won't -> will no<br>\n",
    "there're -> there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w6OrEaYZAJdi"
   },
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "ItdAoWvlBjdn",
    "outputId": "359b88bb-26c8-4a9b-d97c-283df207dc17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It feels strange to review chips, but I am compelled because these are my absolute favorite chips. Kettle has lots of flavors but I always go back to these regular lightly salted.<br /><br />They are thicker and sometimes greasy at the bottom of the bag, plus they often looked burned. They are not actually burned. It has something to do with how much sugar is in the potato that makes it dark. I love to open a bag and find some browned chips.<br /><br />Lays and Wise have their place but I've become a chip snob thanks to Kettle.<br /><br />Like Edy's or Breyer's ice cream, some products are worth the money. These are the best of the best.\n",
      "It took a while to get used to these chips but they are pretty good and more healthy for you. They sure were eaten all up by my family and friends.\n",
      "If you want to find a good flavor substitute for maple syrup, barley malt, or brown rice syrup, this will do nicely. I use Madhava's agave nectar in my baked goods all the time, and am always pleased with the results. At the time I'm writing this, Amazon was out of stock as a vendor. However, when it was in stock, it was priced well for the 2-pack.<br /><br />Other than baking, I've used it to top waffles, pancakes, and in small doses (1/2 teaspoon) added it to oatmeal. A little goes a long way.<br /><br />Using agave nectar: Use 1/3 to 1/2 less agave nectar than you would another sweetener (depending on your taste and the recipe. Recipes with dried fruit need much less), cut the heat in the oven by 25 degrees F; if there is oil in the recipe, cut it by 1/3 (alternately, use a tablespoon or two of sifted coconut flour to absorb more of the liquid -- just don't use too much or your recipe will be dry). Additionally, when baking, make sure to mix it with the wet ingredients prior to adding it to the dry. Most bakers might already know this, but remember to read recipes first because sugar is typically added with dry ingredients (unless beaten with butter first).<br /><br />Agave nectar does not crystalize like other sugars do, so you won't be able to substitute it for corn syrup in making hard candy. However, I highly recommend it for baking. It makes everything moister.<br /><br />If you are looking for less caramel flavor, you'll need to use the light version of this product (equally good in all applications).\n",
      "This stuff is great because it's low glycemic. Substitute this to sugar and you'll be doing your body a great favor.  This size is economical and shipping is fast, too.  I got mine very soon.\n",
      "After looking at the pictures someone put on here showing a crushed box, I had to write a review.  I have bought these chips numerous times from Amazon Warehouse Deals and each time they came packaged perfectly.  There was a sale at the end of the summer and I received a few cases/boxes for just over $10.00.  The chips were fresh and very hard to eat just one bag.  I hid them in the garage and only remembered them when I went out there; that way I didn't eat them all.  As far as the pictures, if the person contacted Amazon, he made out ok.  Amazon is great with handling complaints and this person was given perfect customer service if he called Amazon.  Thanks for a great tasting product, good price and quick shipping.  I'll buy these again...soon.\n"
     ]
    }
   ],
   "source": [
    "# Without using decontracted function\n",
    "i=0\n",
    "n=[]\n",
    "while i<5:\n",
    "  n.append(random.randint(100,1000))\n",
    "  print(text.values[n[i]])\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iHrUeD4ZG2ev"
   },
   "source": [
    "Here we also see  numbers so we need to remove them too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "xFSLf1frCMPL",
    "outputId": "f9639800-99fe-4208-9632-a781bf95a28e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it feels strange to review chips but i am compelled because these are my absolute favorite chips kettle has lots of flavors but i always go back to these regular lightly saltedthey are thicker and sometimes greasy at the bottom of the bag plus they often looked burned they are not actually burned it has something to do with how much sugar is in the potato that makes it dark i love to open a bag and find some browned chipslays and wise have their place but ive become a chip snob thanks to kettlelike edys or breyers ice cream some products are worth the money these are the best of the best\n",
      "it took a while to get used to these chips but they are pretty good and more healthy for you they sure were eaten all up by my family and friends\n",
      "if you want to find a good flavor substitute for maple syrup barley malt or brown rice syrup this will do nicely i use madhavas agave nectar in my baked goods all the time and am always pleased with the results at the time im writing this amazon was out of stock as a vendor however when it was in stock it was priced well for the packother than baking ive used it to top waffles pancakes and in small doses  teaspoon added it to oatmeal a little goes a long wayusing agave nectar use  to  less agave nectar than you would another sweetener depending on your taste and the recipe recipes with dried fruit need much less cut the heat in the oven by  degrees f if there is oil in the recipe cut it by  alternately use a tablespoon or two of sifted coconut flour to absorb more of the liquid  just dont use too much or your recipe will be dry additionally when baking make sure to mix it with the wet ingredients prior to adding it to the dry most bakers might already know this but remember to read recipes first because sugar is typically added with dry ingredients unless beaten with butter firstagave nectar does not crystalize like other sugars do so you wont be able to substitute it for corn syrup in making hard candy however i highly recommend it for baking it makes everything moisterif you are looking for less caramel flavor youll need to use the light version of this product equally good in all applications\n",
      "this stuff is great because its low glycemic substitute this to sugar and youll be doing your body a great favor  this size is economical and shipping is fast too  i got mine very soon\n",
      "after looking at the pictures someone put on here showing a crushed box i had to write a review  i have bought these chips numerous times from amazon warehouse deals and each time they came packaged perfectly  there was a sale at the end of the summer and i received a few casesboxes for just over   the chips were fresh and very hard to eat just one bag  i hid them in the garage and only remembered them when i went out there that way i didnt eat them all  as far as the pictures if the person contacted amazon he made out ok  amazon is great with handling complaints and this person was given perfect customer service if he called amazon  thanks for a great tasting product good price and quick shipping  ill buy these againsoon\n"
     ]
    }
   ],
   "source": [
    "#after using  all function\n",
    "from bs4 import BeautifulSoup\n",
    "for i in n:\n",
    "  cleanHtml= BeautifulSoup(text.values[i], 'lxml').get_text() ## Remove Html Tags\n",
    "  cleanUrl=removeUrl(cleanHtml) #remove\n",
    "  cleanNums = re.sub(r'[0-9]+', '', cleanUrl)   ## To Remove Numbers from the string\n",
    "  low=cleanNums.lower()\n",
    "  cleanChar = re.sub(r'[^a-z0-9\\s]', '', low)   ## To clean all special Charaters\n",
    "  AddCleaned=decontracted(cleanChar)\n",
    "  print(AddCleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2Q_hkTX8T7G"
   },
   "source": [
    "<b>Stemming -></b> Its the process  of reducing inflected words to their word stem, base or root form—generally a written word form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tYcsNDtx8TIG"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BlzfMmz38XYB"
   },
   "outputs": [],
   "source": [
    "snow=nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iClcS7JS9Ya2",
    "outputId": "7aa19dd7-793f-46c4-9510-158f6420057b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hack'"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow.stem('hacked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kegVkEtTQzIb"
   },
   "source": [
    "##Stop Words **Removal**\n",
    "<br>\n",
    "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.<br>\n",
    "<b>Why to remove stopwords?</b>\n",
    "<br>\n",
    "Removing stopwords can potentially help improve the performance as there are fewer and only meaningful tokens left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PUrPqfdP9u7J"
   },
   "outputs": [],
   "source": [
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "ZvvevdG2PM8A",
    "outputId": "97828800-328a-444b-8fd0-03dc9e4b968d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text With Stop Words\n",
      "--------------------------------------------------------------------------------\n",
      "All my fault. I thought this would be a carton of 3 boxes of cereal.  Instead it was 1 box of cereal for 12.99. Who sells cereal for this price plus $8.00 shipping ? I feel stupid!  Buyer Beware!\n"
     ]
    }
   ],
   "source": [
    "print(\"Text With Stop Words\")\n",
    "sample=final.Text.values[1000]\n",
    "print(\"-\"*80)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bE5Bh_AYQvmT"
   },
   "source": [
    "Lets see how to remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "8ZRR8-_bPkTX",
    "outputId": "815073c3-4f7d-42fb-af0a-e203e4d4b46c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Removing StopWords\n",
      "--------------------------------------------------------------------------------\n",
      "All fault. I thought would carton 3 boxes cereal.  Instead 1 box cereal 12.99. Who sells cereal price plus $8.00 shipping ? I feel stupid!  Buyer Beware!\n"
     ]
    }
   ],
   "source": [
    "print(\"After Removing StopWords\")\n",
    "print(\"-\"*80)\n",
    "token=sample.split(\" \")   ## coverting string to token (list of word) \\\\ like [\"this\",\"is\",\"token\"]\n",
    "removestop=[x for x in token if x not in stopwords]   ##removing all the words from the token that are stopwords\n",
    "removed=\" \".join(removestop)  ##joing back the list into sentence\n",
    "print(removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ms6_ELk3S-3F"
   },
   "source": [
    "Now lets just write a small function to remove <b>Stopword and do Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWKM-jfATHtB"
   },
   "outputs": [],
   "source": [
    "def removeStopWord(word):\n",
    "  token=word.split(\" \")   ## coverting string to token (list of word) \\\\ like [\"this\",\"is\",\"token\"]\n",
    "  removestop=[snow.stem(x) for x in token if x not in stopwords]   ##removing stopwords and also doing Stemming\n",
    "  removed=\" \".join(removestop)  ##joing back the list into sentence\n",
    "  return removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8X0SbO7UPpN"
   },
   "source": [
    "**Now lets just create a list containg all cleaned text after applying all the filter that we have discussed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yIcovuRIEaaI",
    "outputId": "a9d0ce71-9857-46da-d267-5ed4bdd4e365"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524505/524505 [07:31<00:00, 1162.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_reviews = []\n",
    "for line in tqdm(final.Text.values):\n",
    "  line= BeautifulSoup(line, 'lxml').get_text() ## Remove Html Tags\n",
    "  line=removeUrl(line) #removing url\n",
    "  line=decontracted(line)    #Coverting word like { are't -> are not}\n",
    "  line = re.sub(r'[0-9]+', '', line)   ## To Remove Numbers from the string\n",
    "  line=line.lower()   ## Converting every word to lower case\n",
    "  line = re.sub(r'[^a-z0-9\\s]', '', line)   ## To clean all special Charaters\n",
    "  line=removeStopWord(line)    ## Removing Stop Words And doing Steaming\n",
    "  preprocessed_reviews.append(line.strip()) ## ading cleaned word into a list after removing spaces {By using strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MMAjG28n9DSf",
    "outputId": "5a8a67b8-c02e-4285-c741-60877af5eae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed list size =  524505\n",
      "Normal list size =  524505\n"
     ]
    }
   ],
   "source": [
    "#just to check evry thing went correctly after pre-processing\n",
    "print(\"preprocessed list size = \",len(preprocessed_reviews))\n",
    "print(\"Normal list size = \",final.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cv4ba4Nhwmlx"
   },
   "source": [
    "# [4]Featurization<br>\n",
    "<b>Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuCWeLeYxGF1"
   },
   "source": [
    "Lets just see with a toy example how theBag Of Words actually looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_dtpMmTSwyKZ",
    "outputId": "6733b81b-9943-48a0-ca32-8be2bf2e1748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "## uni-gram\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "YoD6y8HJxC1P",
    "outputId": "8de90d03-4586-4041-f224-3b633a038b75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'and this', 'document', 'document is', 'first', 'first document', 'is', 'is the', 'is this', 'one', 'second', 'second document', 'the', 'the first', 'the second', 'the third', 'third', 'third one', 'this', 'this document', 'this is', 'this the']\n",
      "(4, 22)\n"
     ]
    }
   ],
   "source": [
    "## by-gram\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUYazQmhBsY0"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WSQ7EY2Q4wgx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection  import cross_val_score\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOIQkqA8Bxmi"
   },
   "source": [
    "Spliting data it into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4e8ulbhU1xR0"
   },
   "outputs": [],
   "source": [
    "X=preprocessed_reviews\n",
    "\n",
    "y=np.array(final['Score'])\n",
    "X_1, X_test, y_1, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_1, y_1, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "QyqX4zvd_HWu",
    "outputId": "2dc531e5-b74d-44e8-f6fb-f13415141ccc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tri waffl mix bought new waffl maker amaz tasti waffl would go bother measur ingredi small number waffl use mix besid conveni waffl absolut delici not imagin not mix hand pantri definit winner'"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHV3PDRAB9Fb"
   },
   "source": [
    "## Using Bag of Word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nlckuFnM2Oqx"
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train=count_vect.fit_transform(X_train)\n",
    "X_cv=count_vect.transform(X_cv)\n",
    "X_test=count_vect.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOFAnkAQCRqo"
   },
   "source": [
    "### Using Standard Scaler to  Standardize features by removing the mean and scaling to unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TclKGJT5kcg"
   },
   "outputs": [],
   "source": [
    "scalar = StandardScaler(with_mean=False)\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test= scalar.transform(X_test)\n",
    "X_cv=scalar.transform(X_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPKhRLGPAM94"
   },
   "source": [
    "# Model Building\n",
    "## Logestic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvUyYrwZ-SgY"
   },
   "outputs": [],
   "source": [
    "c = 10**-4\n",
    "lr=LogisticRegression(penalty='l2',C=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "iG-t65L5AHL8",
    "outputId": "3d471032-b7b4-4dda-cca0-1b7df1c272ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J425Jhk9Aqn_"
   },
   "outputs": [],
   "source": [
    "probcv=lr.predict_proba(X_cv)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KvikIYNNHkfO",
    "outputId": "bc51bca8-74eb-46fe-c4df-f5e282b8ac5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.7818346168856577\n"
     ]
    }
   ],
   "source": [
    "predictions = lr.predict(X_cv)\n",
    "print('AUC: ', roc_auc_score(y_cv, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPxmKIAgCx2S"
   },
   "source": [
    "## Mode have Accurracy of 78 % lets try to increses this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSwczfbeJSZ3"
   },
   "source": [
    "DOing Same Opereartion Using Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UWZRjUBnI-a1"
   },
   "outputs": [],
   "source": [
    "X=preprocessed_reviews\n",
    "\n",
    "y=np.array(final['Score'])\n",
    "X_1, X_test, y_1, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_1, y_1, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "prm2QtkOLhKg",
    "outputId": "e7dc26eb-2e17-43e7-cf27-c5cc39995a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy dogswel chew treat dog sever year alway love recent time purchas howev got serious ill heavi vomit lethargi almost forgot week later found back cabinet tri got ill especi sinc rare sick difficult ignor link treat ill  unless qualiti control high recommend find differ treat issu  amad china look packag care read review factth packag clear state product made china find pretti incredul consid claim packag natur even free rang defin bewar\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X_test[5])\n",
    "print(y_test[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "peB5vt3xDAQP"
   },
   "source": [
    "### Using Bi-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufP-1z9hJ8-9"
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(min_df = 5, ngram_range = (1,2))\n",
    "X_train=count_vect.fit_transform(X_train)\n",
    "X_cv=count_vect.transform(X_cv)\n",
    "X_test=count_vect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BslSmeIjKJUQ"
   },
   "outputs": [],
   "source": [
    "scalar = StandardScaler(with_mean=False)\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test= scalar.transform(X_test)\n",
    "X_cv=scalar.transform(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "CG7F2LsFKXNo",
    "outputId": "94a4c146-80dc-4a32-a94c-acd141a25f1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 106,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LogisticRegression()\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xAWXSXDUKoc6",
    "outputId": "75362be9-f9d3-4a31-a276-3c72e5f69225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8851395124283175\n"
     ]
    }
   ],
   "source": [
    "predictions = lr.predict(X_test)\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjdZ9nUmDJUM"
   },
   "source": [
    "### Now we get pretty good acuraccy of 88%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-tcBCXvDSln"
   },
   "source": [
    "#### Lets just check it mannuly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6wfHncpoKvrQ"
   },
   "outputs": [],
   "source": [
    "text=['I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most','Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".']\n",
    "text=count_vect.transform(text)\n",
    "text=scalar.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iTFCXfWrLKNZ",
    "outputId": "e7855eb1-ff8a-4cbd-a447-9551f24bccf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(lr.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pnn8Q86sTi1M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Amazon Food Review Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
